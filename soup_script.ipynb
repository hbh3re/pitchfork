{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, base_url, num_pages):\n",
    "        self.base_url = base_url\n",
    "        self.num_pages = num_pages\n",
    "        self.cur_page  = 1\n",
    "        self.website = 'https://pitchfork.com'\n",
    "        self.data = []\n",
    "\n",
    "    def parse(self):\n",
    "        # loop through every page of Pitchfork reviews\n",
    "        for i in tqdm(range(1, self.num_pages+1)):\n",
    "            url = self.base_url + str(i)\n",
    "            web_page = requests.get(url)\n",
    "            soup = BeautifulSoup(web_page.content, 'lxml')\n",
    "            # loop through each review on the page (usually 12)\n",
    "            for review in soup.find_all('div', class_='review'):\n",
    "                # get each review link, get the page and scrape it using parse_review function\n",
    "                rev_ext = review.find('a', class_='review__link')['href']\n",
    "                rev_url = self.website + rev_ext\n",
    "                rev_page = requests.get(rev_url)\n",
    "                rev_data = self.parse_review(rev_page)\n",
    "                # save the returned data \n",
    "                self.data.append(rev_data)\n",
    "\n",
    "            if self.cur_page % 100 == 0:\n",
    "                print('%d pages scraped' % self.cur_page)\n",
    "            self.cur_page = self.cur_page + 1\n",
    "            \n",
    "        print('Scraping is complete!')\n",
    "\n",
    "\n",
    "    # get all relevant data from the review\n",
    "    def parse_review(self, rev_page):\n",
    "        rev_data = {}\n",
    "        rev_soup = BeautifulSoup(rev_page.content, 'lxml')\n",
    "        # album\n",
    "        album = rev_soup.find('h1', class_='single-album-tombstone__review-title')\n",
    "        if album: # cathes occassional error where album name wasn't listed\n",
    "            rev_data['album'] = album.string\n",
    "        else:\n",
    "            rev_data['album'] = 'N/A'\n",
    "        # artists\n",
    "        artists = rev_soup.find('ul', class_='artist-links artist-list single-album-tombstone__artist-links')\n",
    "        if artists: # catching error if no artists are found\n",
    "            artists = artists.find_all('a')\n",
    "            artist_list = []\n",
    "            for artist in artists: # loop to check if there is more than one artist\n",
    "                artist_list.append(artist.string)\n",
    "            rev_data['artist'] = ', '.join(artist_list)\n",
    "        else:\n",
    "            rev_data['artist'] = 'Various Artists'\n",
    "        # score\n",
    "        rev_data['score'] = float(rev_soup.find('span', class_='score').string)\n",
    "        # date\n",
    "        rev_data['date'] = rev_soup.find('time', class_='pub-date').string\n",
    "        # reviewer\n",
    "        rev_data['reviewer'] = rev_soup.find('a', class_='authors-detail__display-name').string\n",
    "        # best new music\n",
    "        bnm = rev_soup.find('p', class_='bnm-txt')\n",
    "        if bnm and bnm.string == 'Best new music':\n",
    "            rev_data['bnm'] = 1\n",
    "        else:\n",
    "            rev_data['bnm'] = 0\n",
    "        # text\n",
    "        paragraphs = rev_soup.find('div', class_='contents dropcap')\n",
    "        if paragraphs:\n",
    "            paragraphs = paragraphs.find_all('p')\n",
    "            paragraph_list = []\n",
    "            for paragraph in paragraphs:\n",
    "                paragraph_list.append(''.join(list(paragraph.strings)))\n",
    "            rev_data['text'] = ' '.join(paragraph_list)\n",
    "        else:\n",
    "            rev_data['text'] = None\n",
    "        # header\n",
    "        rev_data['abstract'] = rev_soup.find('div', class_='review-detail__abstract').p.string\n",
    "        # url\n",
    "        rev_data['url'] = rev_page.url\n",
    "        # are there multiple albums being reviewed at once\n",
    "        if rev_soup.find('nav', class_='album-picker'):\n",
    "            rev_data['mult_albums'] = 1\n",
    "        else:\n",
    "            rev_data['mult_albums'] = 0\n",
    "\n",
    "        return rev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing scraper...\n",
      "Running parse function...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-038546ddce20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running parse function...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpitchfork_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving data to csv...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-449e23c9ed82>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mrev_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebsite\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrev_ext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mrev_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mrev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# save the returned data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-449e23c9ed82>\u001b[0m in \u001b[0;36mparse_review\u001b[0;34m(self, rev_page)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mrev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'artist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Various Artists'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mrev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mrev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrev_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pub-date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "base_url = 'https://pitchfork.com/reviews/albums/?page='\n",
    "total_pages = 1681\n",
    "print('Initializing scraper...')\n",
    "pitchfork_scraper = Scraper(base_url, total_pages)\n",
    "\n",
    "print('Running parse function...')\n",
    "pitchfork_scraper.parse()\n",
    "\n",
    "print('Saving data to csv...')\n",
    "final_data = pd.DataFrame(pitchfork_scraper.data)\n",
    "final_data.to_csv('pitchfork_reviews_data.csv')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "20bc9c6a6da244f18765f02d2dfc020a": {
     "views": [
      {
       "cell_index": 1
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
